{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtKx-OpdPWlV",
        "outputId": "26022fbe-1450-4b55-d15e-97e450f3f6fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-21 07:41:50--  https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv\n",
            "Resolving lazyprogrammer.me (lazyprogrammer.me)... 172.67.213.166, 104.21.23.210, 2606:4700:3031::6815:17d2, ...\n",
            "Connecting to lazyprogrammer.me (lazyprogrammer.me)|172.67.213.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5085081 (4.8M) [text/csv]\n",
            "Saving to: ‘bbc_text_cls.csv’\n",
            "\n",
            "bbc_text_cls.csv    100%[===================>]   4.85M  5.22MB/s    in 0.9s    \n",
            "\n",
            "2023-01-21 07:41:52 (5.22 MB/s) - ‘bbc_text_cls.csv’ saved [5085081/5085081]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "ZJablQthRnNz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlZTzc-9Rvs7",
        "outputId": "eec0feae-60fb-48a2-a2f2-7a217284dca5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('bbc_text_cls.csv')"
      ],
      "metadata": {
        "id": "1InWLbW1Ry4K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "fwMdcKQhR34u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# populate word2idx\n",
        "idx = 0\n",
        "word2idx = {}\n",
        "tokenized_docs = []\n",
        "for doc in df['text']:\n",
        "  words = word_tokenize(doc.lower())\n",
        "  doc_as_int = []\n",
        "  for word in words:\n",
        "    if word not in  word2idx:\n",
        "      word2idx[word] = idx\n",
        "      idx += 1\n",
        "\n",
        "    doc_as_int.append(word2idx[word])\n",
        "  tokenized_docs.append(doc_as_int)"
      ],
      "metadata": {
        "id": "ljpButtOR5I3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse mapping\n",
        "idx2word = {v:k for k, v in word2idx.items()}"
      ],
      "metadata": {
        "id": "zzEAzt2fTdC1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of documents\n",
        "N = len(df['text'])"
      ],
      "metadata": {
        "id": "6kX0xZ8XTdAR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of words\n",
        "V = len(word2idx)"
      ],
      "metadata": {
        "id": "k2w96MRCTc9e"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate term-frequency matrix\n",
        "tf = np.zeros((N, V))"
      ],
      "metadata": {
        "id": "Mf-TqKu6Tc6l"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# populate term-frequency counts\n",
        "for i, doc_as_int in enumerate(tokenized_docs):\n",
        "  for j in doc_as_int:\n",
        "    tf[i, j] += 1"
      ],
      "metadata": {
        "id": "SPRAXQ4YTc0e"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute IDF\n",
        "document_freq = np.sum(tf>0, axis=0) # documnet frequency\n",
        "idf = np.log(N/document_freq)"
      ],
      "metadata": {
        "id": "wy0tofqhTcmv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute TF-IDF\n",
        "tf_idf = tf * idf"
      ],
      "metadata": {
        "id": "Bm82hwGvUj8v"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)"
      ],
      "metadata": {
        "id": "NM1SBEh0Upl1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick a random document\n",
        "i = np.random.choice(N)\n",
        "row = df.iloc[i]\n",
        "print(\"Label:\", row['labels'])\n",
        "print(\"Text:\", row['text'].split(\"\\n\", 1)[0])\n",
        "print(\"Top 5 terms:\")\n",
        "\n",
        "scores = tf_idf[i]\n",
        "indices = (-scores).argsort()\n",
        "\n",
        "for j in indices[:5]:\n",
        "  print(idx2word[j])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os78GH1AUrce",
        "outputId": "83f7102c-d843-41da-b876-44bb8d287bc8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: tech\n",
            "Text: IBM puts cash behind Linux push\n",
            "Top 5 terms:\n",
            "linux\n",
            "ibm\n",
            "workplace\n",
            "software\n",
            "programs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gm2tYZm_Vp4d"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}